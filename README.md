# American Sign Language to Written Words Translator

This project aims to develop a computer vision system that can identify American Sign Language (ASL) gestures and translate them into written words. The primary objective of this project is to utilize image classification techniques to recognize ASL signs and convert them into corresponding text.

## Problem Statement

Google aimed to create a system similar to Google Translate, but specifically for translating American Sign Language gestures into written words. This involves developing an accurate and efficient image classifier that can understand the diverse range of hand signs and gestures used in ASL and then convert them into understandable text.

## Objective

The main objective of this project is to leverage computer vision techniques to accurately identify and translate American Sign Language gestures into written words. By creating an image classifier that can distinguish between different ASL signs and associate them with their corresponding textual representation, this project strives to bridge the communication gap between ASL users and non-users.

The project will involve tasks such as:
- Collecting and preparing a dataset of ASL gesture images.
- Designing and training an image classification model.
- Evaluating the model's accuracy and performance on ASL gesture recognition.
- Implementing the translation mechanism to convert recognized gestures into written words.

The successful completion of this project would contribute to improving communication and accessibility for people who use ASL as their primary means of communication.

## Technologies Used

The project will make use of various tools and technologies including:
- Python programming language
- Computer vision libraries (e.g., OpenCV)
- Deep learning frameworks (e.g., TensorFlow)
- Data preprocessing techniques
- Image classification algorithms
